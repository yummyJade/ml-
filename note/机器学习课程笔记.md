# 机器学习

来源于吴老师的课程

**一定要多动手实践啊**

[TOC]



## 多变量线性回归

![image-20200728141011176](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728141011176.png)

其式子其实是等价的



### 特征缩放 feature scaling

其想法是使得多个特征值的取值范围相似，进而实现更快的收敛

ex

![image-20200728141524210](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728141524210.png)

更通用的表达：

get every feature into approximately a -1<=xi<=1 range1

但是注意这里的[-1,1]并非是精确的概念

ex 

[0, 3]  和 [-2, 0.5]是可以的

[0, 3]和[-100, 100]是不可的



归一化

![image-20200728142247989](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728142247989.png)

mu2：x2的平均值

s2：标准差或者(max-min)，两个方式所得范围是差不多的



总结：特征缩放可以让梯度下降的过程更快更好，在具体使用时不需要特别精确



### 学习率 learning rate

如何确保梯度下降正常工作？

![image-20200728142846735](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728142846735.png)

图中第一个红点的意义是完成100次迭代之后得到的theta代入求得的代价函数

对本图的解读：

* 随着迭代次数增加，代价函数J(theta)应该逐渐变小
* 当曲线逐渐平坦时(flat)，梯度下降算法可以视为收敛

其他判断收敛的方式：自动收敛测试，当代价函数小于一个阈值(threshold)时视为收敛，但阈值的选择是困难的

其他不正常的情况：

![](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728143627185.png)

可能是学习率alpha过大造成的，选择较小的alpha



如何选择learning rate？

尝试多个alpha

![image-20200728143813320](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728143813320.png)



### 特征和多项式回归

ex

对于房屋与房价的关系模型，我们不单独考虑frontage和depth两个单独的特征，而是选择构建一个新特性area=frontage*depth作为方程中的参数



在某些时候，你可能会定义自己的新特征，可能这样会得到更好的模型



![image-20200728144955131](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728144955131.png)

或者面对这样的数据，二次函数不能够很好的描述这些点，毕竟价格是不会随着size增加而下降的，这时候你可能会根据自己的经验构造一个新的函数并去做拟合



所以两个问题：如何选择特征？如何选择函数？



### 正规方程 normal equation

给出了不需要多次迭代一次求解最小值的方式（cool，这不是线代里求最值的正规式嘛？！）



对于一个单个参数来说，求导很容易，但是对于多个参数来说，逐个求偏导置0的运算量太大了

![image-20200728151248027](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728151248027.png)



其组织行为如下：

对于一个训练样本x^(i)，有X矩阵如图所示

![image-20200728151621513](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728151621513.png)

![image-20200728152022748](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728152022748.png)

这是最小二乘法 aooaoaoao好像懂了

![EB68827B568CBED25F81DE9F948117E3](D:\downloads\Tencent\MobileFile\EB68827B568CBED25F81DE9F948117E3.png)

参考博客做了一下推导

使用正规式的方式不需要考虑feature scaling

正规方程和梯度下降方式的比较：

|         梯度下降         |                           正规方程                           |
| :----------------------: | :----------------------------------------------------------: |
|      需要选择alpha       |                          不需要选择                          |
|       需要多次迭代       |                        不需要多次迭代                        |
| 当n很大时也可以work well | 需要计算(X T X)^-1，其维度为nxn，计算逆矩阵O(n^3)，当n很大(上万)时慢 |



总结：正规方程与梯度下降都各有优劣，需要根据具体算法具体情况而定，不过对于线性回归，正规方程确实是一个比较好的解决方式



当XTX不可逆：这个学过线代的应该会有一些概念了，比如说存在线性相关的列向量等等



## 逻辑回归

### 假设表示

假设函数：使用g(x)使得h(x)的取值在[0,1]之间

![image-20200729092457814](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729092457814.png)

g(z) sigmod func

h(x)输出的意义是输入为x时y=1的估计概率，即P(y=1|x;theta)



### 决策边界 Decision boundary

假设函数属性之一，**是假设本身及其参数的属性**，取决于假设函数的参数，训练集只是用来拟合

![image-20200729093328548](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729093328548.png)

此时预测y=1的条件是-3+x1+x3>=0，整理得到
$$
x1+x3=3
$$
规定概率大于0.5，y取1

该条直线上的点hx为0.5，直线上方为（预测）y=1区域，下方为y=0区域



### 代价函数  cost function

如何拟合参数theta？

![image-20200729094612527](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729094612527.png)

![image-20200729101529449](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729101529449.png)

若采用线性回归中的模型，由于hx是非线性函数，如果用平方函数计算代价，这会导致 J(theta)成为非凸函数，即存在许多局部最小值，无法梯度下降计算

![image-20200729101255601](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729101255601.png)



![image-20200729101550680](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729101550680.png)

观察y=1时的图像

![image-20200729102040514](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729102040514.png)



当hx=1时，即真实值和预测值一致，此时代价为0

当hx=0时，即真实值和预测值完全相反，错误严重，此时代价最大



y=0时同理

![image-20200729102301679](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729102301679.png)



一种更简单的代价函数表达形式，注意y只有取值{0,1}，其中hx是预测值，y是真实值
$$
Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))
$$
故有
$$
\begin{align}
J(\theta)&=\frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)},y^{(i)})\\
&=-\frac{1}{m}[\sum_{i=1}^{m}y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]
\end{align}
$$


基于极大似然估计



### ？梯度下降

其梯度下降算法如下
$$
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)\\
代入J（\theta）求偏导化简即\\
\theta_j:=\theta_j-\alpha\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$
形式上与线性回归一致，但是hx所代表的含义是不同的

！！！注意这里的theta更新过程是可以用向量化而非loop直接实现的，但是昨天那个为啥子算不对，有点坑爹

特征缩放在这里也是适用的



更高级的计算方法

* gradient descent
* conjugate gradient
* BFGS
* L-BFGS

其优势在于：不需要自己选择alpha，收敛速度比梯度方法更快



### 多元回归

即分类结果不只有{0,1}

ex：对于天气问题来说，我们可以分类为Sunny,Cloudy,Rain,Snow

y={1,2,3,4}

![image-20200729114219029](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729114219029.png)



其思想就是将问题转换为3个二元分类问题，生成了三个分类器



## 正则化

### 过拟合 overfit

线性回归

![image-20200729170318432](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729170318432.png)

(1)欠拟合 高偏差high bias

(2)fit

(3)过拟合 高方差high variance：高阶多项式虽然能很好的拟合数据，但是变量过多，数据量不足，很难泛化(generalize)到新样本中



逻辑回归

![image-20200729170641651](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729170641651.png)

解决方法：

* 减少变量：但有可能丢失信息
  * 人工选择
  * 模型选择算法model selection

* 正则化：减少量级



### 代价函数

正则化的思想是引入惩罚 penalize

修改代价函数，使得到的结果中模型参数尽可能的小

ex：预测房子价格

![image-20200729171513064](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729171513064.png)

无法得知100项特征中哪个是有用的，因此选择缩小所有的参数

修改代价函数得到：
$$
J(\theta)=\frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{i=1}^{m}\theta_j^2]
$$
注意我们的惩罚对象是theta 1到theta n，没有theta0

lambda是正则化参数，其目标为：

* 尽可能拟合数据
* 令参数尽可能的小

### 线性回归的正则化



梯度下降

因为不对theta0处理，因此这一项提取出单独考虑



![image-20200730092918239](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200730092918239.png)

其中1-alpha*lambda/m比1略小



正规方程

设计向量X和y

![image-20200730093258450](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200730093258450.png)

### 逻辑回归的正则化

梯度下降

![image-20200730093817726](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200730093817726.png)

方括号中是我们新定义的代价函数求偏导

 

高级算法





## 存疑

* 极大似然估计部分后期可以补一下

* 凸优化

* ！！！注意这里的theta更新过程是可以用向量化而非loop直接实现的，但是昨天那个为啥子算不对，有点坑爹

  



参考

> ​	https://blog.csdn.net/qq_29317617/article/details/86312154



















