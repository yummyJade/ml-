# 机器学习

来源于吴老师的课程

**一定要多动手实践啊**

[TOC]

## 多变量线性回归

![image-20200728141011176](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728141011176.png)

其式子其实是等价的



### 特征缩放 feature scaling

其想法是使得多个特征值的取值范围相似，进而实现更快的收敛

ex

![image-20200728141524210](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728141524210.png)

更通用的表达：

get every feature into approximately a -1<=xi<=1 range1

但是注意这里的[-1,1]并非是精确的概念

ex 

[0, 3]  和 [-2, 0.5]是可以的

[0, 3]和[-100, 100]是不可的



归一化

![image-20200728142247989](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728142247989.png)

mu2：x2的平均值

s2：标准差或者(max-min)，两个方式所得范围是差不多的



总结：特征缩放可以让梯度下降的过程更快更好，在具体使用时不需要特别精确



### 学习率 learning rate

如何确保梯度下降正常工作？

![image-20200728142846735](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728142846735.png)

图中第一个红点的意义是完成100次迭代之后得到的theta代入求得的代价函数

对本图的解读：

* 随着迭代次数增加，代价函数J(theta)应该逐渐变小
* 当曲线逐渐平坦时(flat)，梯度下降算法可以视为收敛

其他判断收敛的方式：自动收敛测试，当代价函数小于一个阈值(threshold)时视为收敛，但阈值的选择是困难的

其他不正常的情况：

![](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728143627185.png)

可能是学习率alpha过大造成的，选择较小的alpha



如何选择learning rate？

尝试多个alpha

![image-20200728143813320](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728143813320.png)



### 特征和多项式回归

ex

对于房屋与房价的关系模型，我们不单独考虑frontage和depth两个单独的特征，而是选择构建一个新特性area=frontage*depth作为方程中的参数



在某些时候，你可能会定义自己的新特征，可能这样会得到更好的模型



![image-20200728144955131](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728144955131.png)

或者面对这样的数据，二次函数不能够很好的描述这些点，毕竟价格是不会随着size增加而下降的，这时候你可能会根据自己的经验构造一个新的函数并去做拟合



所以两个问题：如何选择特征？如何选择函数？



### 正规方程 normal equation

给出了不需要多次迭代一次求解最小值的方式（cool，这不是线代里求最值的正规式嘛？！）



对于一个单个参数来说，求导很容易，但是对于多个参数来说，逐个求偏导置0的运算量太大了

![image-20200728151248027](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728151248027.png)



其组织行为如下：

对于一个训练样本x^(i)，有X矩阵如图所示

![image-20200728151621513](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728151621513.png)

![image-20200728152022748](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200728152022748.png)

这是最小二乘法 aooaoaoao好像懂了

![EB68827B568CBED25F81DE9F948117E3](D:\downloads\Tencent\MobileFile\EB68827B568CBED25F81DE9F948117E3.png)

参考博客做了一下推导

使用正规式的方式不需要考虑feature scaling

正规方程和梯度下降方式的比较：

|         梯度下降         |                           正规方程                           |
| :----------------------: | :----------------------------------------------------------: |
|      需要选择alpha       |                          不需要选择                          |
|       需要多次迭代       |                        不需要多次迭代                        |
| 当n很大时也可以work well | 需要计算(X T X)^-1，其维度为nxn，计算逆矩阵O(n^3)，当n很大(上万)时慢 |



总结：正规方程与梯度下降都各有优劣，需要根据具体算法具体情况而定，不过对于线性回归，正规方程确实是一个比较好的解决方式



当XTX不可逆：这个学过线代的应该会有一些概念了，比如说存在线性相关的列向量等等



## 逻辑回归

### 假设表示

假设函数：使用g(x)使得h(x)的取值在[0,1]之间

![image-20200729092457814](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729092457814.png)

g(z) sigmod func

h(x)输出的意义是输入为x时y=1的估计概率，即P(y=1|x;theta)



### 决策边界 Decision boundary

假设函数属性之一，**是假设本身及其参数的属性**，取决于假设函数的参数，训练集只是用来拟合

![image-20200729093328548](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729093328548.png)

此时预测y=1的条件是-3+x1+x3>=0，整理得到
$$
x1+x3=3
$$
规定概率大于0.5，y取1

该条直线上的点hx为0.5，直线上方为（预测）y=1区域，下方为y=0区域



### 代价函数  cost function

如何拟合参数theta？

![image-20200729094612527](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729094612527.png)

![image-20200729101529449](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729101529449.png)

若采用线性回归中的模型，由于hx是非线性函数，如果用平方函数计算代价，这会导致 J(theta)成为非凸函数，即存在许多局部最小值，无法梯度下降计算

![image-20200729101255601](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729101255601.png)



![image-20200729101550680](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729101550680.png)

观察y=1时的图像

![image-20200729102040514](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729102040514.png)



当hx=1时，即真实值和预测值一致，此时代价为0

当hx=0时，即真实值和预测值完全相反，错误严重，此时代价最大



y=0时同理

![image-20200729102301679](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729102301679.png)



一种更简单的代价函数表达形式，注意y只有取值{0,1}，其中hx是预测值，y是真实值
$$
Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))
$$
故有
$$
\begin{align}
J(\theta)&=\frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)},y^{(i)})\\
&=-\frac{1}{m}[\sum_{i=1}^{m}y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]
\end{align}
$$


基于极大似然估计



### ？梯度下降

其梯度下降算法如下
$$
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)\\
代入J（\theta）求偏导化简即\\
\theta_j:=\theta_j-\alpha\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$
形式上与线性回归一致，但是hx所代表的含义是不同的

！！！注意这里的theta更新过程是可以用向量化而非loop直接实现的，但是昨天那个为啥子算不对，有点坑爹

特征缩放在这里也是适用的



更高级的计算方法

* gradient descent
* conjugate gradient
* BFGS
* L-BFGS

其优势在于：不需要自己选择alpha，收敛速度比梯度方法更快



### 多元回归

即分类结果不只有{0,1}

ex：对于天气问题来说，我们可以分类为Sunny,Cloudy,Rain,Snow

y={1,2,3,4}

![image-20200729114219029](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729114219029.png)



其思想就是将问题转换为3个二元分类问题，生成了三个分类器



## 正则化

### 过拟合 overfit

线性回归

![image-20200729170318432](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729170318432.png)

(1)欠拟合 高偏差high bias

(2)fit

(3)过拟合 高方差high variance：高阶多项式虽然能很好的拟合数据，但是变量过多，数据量不足，很难泛化(generalize)到新样本中



逻辑回归

![image-20200729170641651](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729170641651.png)

解决方法：

* 减少变量：但有可能丢失信息
  * 人工选择
  * 模型选择算法model selection

* 正则化：减少量级



### 代价函数

正则化的思想是引入惩罚 penalize

修改代价函数，使得到的结果中模型参数尽可能的小

ex：预测房子价格

![image-20200729171513064](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729171513064.png)

无法得知100项特征中哪个是有用的，因此选择缩小所有的参数

修改代价函数得到：
$$
J(\theta)=\frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{i=1}^{m}\theta_j^2]
$$
注意我们的惩罚对象是theta 1到theta n，没有theta0

lambda是正则化参数，其目标为：

* 尽可能拟合数据
* 令参数尽可能的小

### 线性回归的正则化



梯度下降

因为不对theta0处理，因此这一项提取出单独考虑



![image-20200730092918239](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200730092918239.png)

其中1-alpha*lambda/m比1略小



正规方程

设计向量X和y

![image-20200730093258450](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200730093258450.png)

### 逻辑回归的正则化

梯度下降

![image-20200730093817726](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200730093817726.png)

方括号中是我们新定义的代价函数求偏导

 

高级算法



## 神经网络

为什么需要神经网络？

非线性的分类，很多特征，构造多项式是不现实的

同时在图像中特征非常多，不便于计算



### 模型展示



![image-20200730203340721](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200730203340721.png)

parameters = weight

x0节点：偏置单元，值永远为1

神经网络

![image-20200730203626049](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200730203626049.png)

layer1: input layer

layer2: hidden layer（可以不止一个）

layer3: output layer



一些具体的符号解释：

![image-20200730204846223](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200730204846223.png)

![image-20200730204831710](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200730204831710.png)

该权重矩阵的规模是3x4

![image-20200730204949510](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200730204949510.png)



向量化的计算方式

对照前式

![image-20200731081516266](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731081516266.png)

a^(1)：第一层的其实就是输入x

a^(2)：首先将g()中那一部分视为z，而z就是之前熟悉的theta*x，加上隐藏层以后theta是四维矩阵

a^(3)：最后一层的hx计算也是类似的，可以

为什么神经网络能够更好的实现非线性回归？



![image-20200731081959293](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731081959293.png)

遮去左半部分，剩下这个从layer2到layer3的过程类似于逻辑回归，从三个特征a_1 a_2 a_3 得到了一个某事成功的概率

这个特征不是原来输入的特征x1 x2 x3，而是自己训练的输入，根据theta的不同而不同

![image-20200731082420829](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731082420829.png)

### 具体案例

![image-20200731090832253](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731090832253.png)



* AND 只要不是两个都为1，得到的结果肯定是负数
* NOT  在输入前加一个很大的负权重

![image-20200731090412547](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731090412547.png)

最后

![image-20200731091451436](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731091451436.png)



### 多元分类

ex：手写数字识别，有0-9

![image-20200731091822525](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731091822525.png)

输出层有4个分类器，可以得到是否是某一个类别的概率



![image-20200731091955938](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731091955938.png)



### 代价函数

以分类问题为例

![image-20200731092237694](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731092237694.png)

L：总层数

S_i：某一层的神经元个数（不包含隐藏层）

![image-20200731092625713](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731092625713.png)

两个分类问题：

* 二元分类：输出为{0,1}，S_L=1,K=1

* 多元分类：输出为多维向量，维数即为分类类别个数，S_L=K（k>=3）



代价函数

![image-20200731093106400](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731093106400.png)  

  其实是和逻辑回归类似的，不过一个layer有多个球，因此需要再来一个循环



### 反向传播

令代价函数最小化的算法
$$
J(\theta) = -\frac{1}{m}[\sum_{i=1}^{m}\sum_{k=1}^{K}y_k^{(i)}logh_{\theta}(x^{(i)})_k+(1-y_k^{(i)})log(1-h_{\theta}(x^{(i)})_k]\\
+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{S_l}\sum_{j=1}^{S_{l+1}}
$$
其目标是
$$
\min_{\theta}J(\theta)
$$
需要计算的部分

![image-20200731094154816](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731094154816.png)



**正向传播：得到每个神经元的激活值**

![image-20200731104028210](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731104028210.png)

![image-20200731103957140](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731103957140.png)





**反向传播：求导**

在这里有![image-20200731094549616](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731094549616.png)

这个delta term和我们之前学的a_j^{(l)}是对应的，那个a代表了某一个layer的某一个神经节点，对应的，这个符号就代表了某一个神经节点激活值的误差（激活函数是某一层到某一层的映射，所以说激活值应是某一个点经过激活函数所得的值）





![image-20200731103245042](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731103245042.png)



反向传播是从最后一层开始的，在计算的时候，我们从最后一层即layer4开始

![image-20200731103446053](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731103446053.png)

* delta 4这个式子可以用向量化的方式计算

* delta 3和delta 2中对g的求导可得![image-20200731103533277](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731103533277.png)

* 没有对于delta 1的求解，因为delta 1是我们观察到的，没有误差的

通过delta我们可以很快的得出偏导值（暂时忽略了正则项）

![image-20200731103812061](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731103812061.png)

**进一步解释**

首先再详细解释前向传播

![image-20200731105212034](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731105212034.png)

![CF539F128FC5E0A94B29E2580DD378C5](D:\downloads\Tencent\MobileFile\CF539F128FC5E0A94B29E2580DD378C5.png)



（啊，不过这个权重矩阵几维我还没整理清楚）



![image-20200731110446969](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731110446969.png)

训练样本(x(i),y(i))

为了更好的理解反向传播，这里以二元分类为例，此时代价函数里对K的求和就可以省略（K=1），同时y={0,1}，故这里的向量y实际上是实数，再进一步忽略正则化，置lambda=0，此时代价函数简化

那么第i个样本的代价为（相当于预测样本值和真值的相近程度）：

![image-20200731110946096](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731110946096.png)



而反向传播中的delta实际上是代价函数关于z的偏导数![image-20200731111227956](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731111227956.png)





![F5BF54CB480DBF2A08D843DF1CD67081](D:\downloads\Tencent\MobileFile\F5BF54CB480DBF2A08D843DF1CD67081.png)

可见反向传播中并不需要考虑偏置项



### 梯度检测 





## 存疑

* 极大似然估计部分后期可以补一下

* 凸优化

* ！！！注意这里的theta更新过程是可以用向量化而非loop直接实现的，但是昨天那个为啥子算不对，有点坑爹

* 实验里面为啥要转矩阵形式，或者说为啥动不动就变成了向量

* 神经网络里那个权重矩阵怎么算的

* 反向传播偏导数

  
  
  



参考

> ​	https://blog.csdn.net/qq_29317617/article/details/86312154



















