# 机器学习

来源于吴老师的课程

**一定要多动手实践啊**

[TOC]

## 多变量线性回归

![image-20200728141011176](../images/image-20200728141011176.png)

其式子其实是等价的



### 特征缩放 feature scaling

其想法是使得多个特征值的取值范围相似，进而实现更快的收敛

ex

![image-20200728141524210](../images/image-20200728141524210.png)

更通用的表达：

get every feature into approximately a -1<=xi<=1 range1

但是注意这里的[-1,1]并非是精确的概念

ex 

[0, 3]  和 [-2, 0.5]是可以的

[0, 3]和[-100, 100]是不可的



归一化

![image-20200728142247989](../images/image-20200728142247989.png)

mu2：x2的平均值

s2：标准差或者(max-min)，两个方式所得范围是差不多的



总结：特征缩放可以让梯度下降的过程更快更好，在具体使用时不需要特别精确



### 学习率 learning rate

如何确保梯度下降正常工作？

![image-20200728142846735](../images/image-20200728142846735.png)

图中第一个红点的意义是完成100次迭代之后得到的theta代入求得的代价函数

对本图的解读：

* 随着迭代次数增加，代价函数J(theta)应该逐渐变小
* 当曲线逐渐平坦时(flat)，梯度下降算法可以视为收敛

其他判断收敛的方式：自动收敛测试，当代价函数小于一个阈值(threshold)时视为收敛，但阈值的选择是困难的

其他不正常的情况：

![](../images/image-20200728143627185.png)

可能是学习率alpha过大造成的，选择较小的alpha



如何选择learning rate？

尝试多个alpha

![image-20200728143813320](../images/image-20200728143813320.png)



### 特征和多项式回归

ex

对于房屋与房价的关系模型，我们不单独考虑frontage和depth两个单独的特征，而是选择构建一个新特性area=frontage*depth作为方程中的参数



在某些时候，你可能会定义自己的新特征，可能这样会得到更好的模型



![image-20200728144955131](../images/image-20200728144955131.png)

或者面对这样的数据，二次函数不能够很好的描述这些点，毕竟价格是不会随着size增加而下降的，这时候你可能会根据自己的经验构造一个新的函数并去做拟合



所以两个问题：如何选择特征？如何选择函数？



### 正规方程 normal equation

给出了不需要多次迭代一次求解最小值的方式（cool，这不是线代里求最值的正规式嘛？！）



对于一个单个参数来说，求导很容易，但是对于多个参数来说，逐个求偏导置0的运算量太大了

![image-20200728151248027](../images/image-20200728151248027.png)



其组织行为如下：

对于一个训练样本x^(i)，有X矩阵如图所示

![image-20200728151621513](../images/image-20200728151621513.png)

![image-20200728152022748](../images/image-20200728152022748.png)

这是最小二乘法 aooaoaoao好像懂了

![EB68827B568CBED25F81DE9F948117E3](../images/EB68827B568CBED25F81DE9F948117E3.png)

参考博客做了一下推导

使用正规式的方式不需要考虑feature scaling

正规方程和梯度下降方式的比较：

|         梯度下降         |                           正规方程                           |
| :----------------------: | :----------------------------------------------------------: |
|      需要选择alpha       |                          不需要选择                          |
|       需要多次迭代       |                        不需要多次迭代                        |
| 当n很大时也可以work well | 需要计算(X T X)^-1，其维度为nxn，计算逆矩阵O(n^3)，当n很大(上万)时慢 |



总结：正规方程与梯度下降都各有优劣，需要根据具体算法具体情况而定，不过对于线性回归，正规方程确实是一个比较好的解决方式



当XTX不可逆：这个学过线代的应该会有一些概念了，比如说存在线性相关的列向量等等



## 逻辑回归

### 假设表示

假设函数：使用g(x)使得h(x)的取值在[0,1]之间

![image-20200729092457814](../images/image-20200729092457814.png)

g(z) sigmod func

h(x)输出的意义是输入为x时y=1的估计概率，即P(y=1|x;theta)



### 决策边界 Decision boundary

假设函数属性之一，**是假设本身及其参数的属性**，取决于假设函数的参数，训练集只是用来拟合

![image-20200729093328548](../images/image-20200729093328548.png)

此时预测y=1的条件是-3+x1+x3>=0，整理得到
$$
x1+x3=3
$$
规定概率大于0.5，y取1

该条直线上的点hx为0.5，直线上方为（预测）y=1区域，下方为y=0区域



### 代价函数  cost function

如何拟合参数theta？

![image-20200729094612527](../images/image-20200729094612527.png)

![image-20200729101529449](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200729101529449.png)

若采用线性回归中的模型，由于hx是非线性函数，如果用平方函数计算代价，这会导致 J(theta)成为非凸函数，即存在许多局部最小值，无法梯度下降计算

![image-20200729101255601](../images/image-20200729101255601.png)



![image-20200729101550680](../images/image-20200729101550680.png)

观察y=1时的图像

![image-20200729102040514](../images/image-20200729102040514.png)



当hx=1时，即真实值和预测值一致，此时代价为0

当hx=0时，即真实值和预测值完全相反，错误严重，此时代价最大



y=0时同理

![image-20200729102301679](../images/image-20200729102301679.png)



一种更简单的代价函数表达形式，注意y只有取值{0,1}，其中hx是预测值，y是真实值
$$
Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))
$$
故有
$$
\begin{align}
J(\theta)&=\frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)},y^{(i)})\\
&=-\frac{1}{m}[\sum_{i=1}^{m}y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]
\end{align}
$$


基于极大似然估计



### ？梯度下降

其梯度下降算法如下
$$
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)\\
代入J（\theta）求偏导化简即\\
\theta_j:=\theta_j-\alpha\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$
形式上与线性回归一致，但是hx所代表的含义是不同的

！！！注意这里的theta更新过程是可以用向量化而非loop直接实现的，但是昨天那个为啥子算不对，有点坑爹

特征缩放在这里也是适用的



更高级的计算方法

* gradient descent
* conjugate gradient
* BFGS
* L-BFGS

其优势在于：不需要自己选择alpha，收敛速度比梯度方法更快



### 多元回归

即分类结果不只有{0,1}

ex：对于天气问题来说，我们可以分类为Sunny,Cloudy,Rain,Snow

y={1,2,3,4}

![image-20200729114219029](../images/image-20200729114219029.png)



其思想就是将问题转换为3个二元分类问题，生成了三个分类器



## 正则化

### 过拟合 overfit

线性回归

![image-20200729170318432](../images/image-20200729170318432.png)

(1)欠拟合 高偏差high bias

(2)fit

(3)过拟合 高方差high variance：高阶多项式虽然能很好的拟合数据，但是变量过多，数据量不足，很难泛化(generalize)到新样本中



逻辑回归

![image-20200729170641651](../images/image-20200729170641651.png)

解决方法：

* 减少变量：但有可能丢失信息
  * 人工选择
  * 模型选择算法model selection

* 正则化：减少量级



### 代价函数

正则化的思想是引入惩罚 penalize

修改代价函数，使得到的结果中模型参数尽可能的小

ex：预测房子价格

![image-20200729171513064](../images/image-20200729171513064.png)

无法得知100项特征中哪个是有用的，因此选择缩小所有的参数

修改代价函数得到：
$$
J(\theta)=\frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{i=1}^{m}\theta_j^2]
$$
注意我们的惩罚对象是theta 1到theta n，没有theta0

lambda是正则化参数，其目标为：

* 尽可能拟合数据
* 令参数尽可能的小

### 线性回归的正则化



梯度下降

因为不对theta0处理，因此这一项提取出单独考虑



![image-20200730092918239](../images/image-20200730092918239.png)

其中1-alpha*lambda/m比1略小



正规方程

设计向量X和y

![image-20200730093258450](../images/image-20200730093258450.png)

### 逻辑回归的正则化

梯度下降

![image-20200730093817726](../images/image-20200730093817726.png)

方括号中是我们新定义的代价函数求偏导

 

高级算法



## 神经网络

为什么需要神经网络？

非线性的分类，很多特征，构造多项式是不现实的

同时在图像中特征非常多，不便于计算



### 模型展示



![image-20200730203340721](../images/image-20200730203340721.png)

parameters = weight

x0节点：偏置单元，值永远为1

神经网络

![image-20200730203626049](../images/image-20200730203626049.png)

layer1: input layer

layer2: hidden layer（可以不止一个）

layer3: output layer



一些具体的符号解释：

![image-20200730204846223](../images/image-20200730204846223.png)

![image-20200730204831710](../images/image-20200730204831710.png)

该权重矩阵的规模是3x4

![image-20200730204949510](../images/image-20200730204949510.png)



向量化的计算方式

对照前式

![image-20200731081516266](../images/image-20200731081516266.png)

a^(1)：第一层的其实就是输入x

a^(2)：首先将g()中那一部分视为z，而z就是之前熟悉的theta*x，加上隐藏层以后theta是四维矩阵

a^(3)：最后一层的hx计算也是类似的，可以

为什么神经网络能够更好的实现非线性回归？



![image-20200731081959293](../images/image-20200731081959293.png)

遮去左半部分，剩下这个从layer2到layer3的过程类似于逻辑回归，从三个特征a_1 a_2 a_3 得到了一个某事成功的概率

这个特征不是原来输入的特征x1 x2 x3，而是自己训练的输入，根据theta的不同而不同

![image-20200731082420829](../images/image-20200731082420829.png)

### 具体案例

![image-20200731090832253](../images/image-20200731090832253.png)



* AND 只要不是两个都为1，得到的结果肯定是负数
* NOT  在输入前加一个很大的负权重

![image-20200731090412547](../images/image-20200731090412547.png)

最后

![image-20200731091451436](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200731091451436.png)



### 多元分类

ex：手写数字识别，有0-9

![image-20200731091822525](../images/image-20200731091822525.png)

输出层有4个分类器，可以得到是否是某一个类别的概率



![image-20200731091955938](../images/image-20200731091955938.png)



### 代价函数

以分类问题为例

![image-20200731092237694](../images/image-20200731092237694.png)

L：总层数

S_i：某一层的神经元个数（不包含隐藏层）

![image-20200731092625713](../images/image-20200731092625713.png)

两个分类问题：

* 二元分类：输出为{0,1}，S_L=1,K=1

* 多元分类：输出为多维向量，维数即为分类类别个数，S_L=K（k>=3）



代价函数

![image-20200731093106400](../images/image-20200731093106400.png)  

  其实是和逻辑回归类似的，不过一个layer有多个球，因此需要再来一个循环



### 反向传播

令代价函数最小化的算法
$$
J(\theta) = -\frac{1}{m}[\sum_{i=1}^{m}\sum_{k=1}^{K}y_k^{(i)}logh_{\theta}(x^{(i)})_k+(1-y_k^{(i)})log(1-h_{\theta}(x^{(i)})_k]\\
+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{S_l}\sum_{j=1}^{S_{l+1}}
$$
其目标是
$$
\min_{\theta}J(\theta)
$$
需要计算的部分

![image-20200731094154816](../images/image-20200731094154816.png)



**正向传播：得到每个神经元的激活值**

![image-20200731104028210](../images/image-20200731104028210.png)

![image-20200731103957140](../images/image-20200731103957140.png)





**反向传播：求导**

在这里有![image-20200731094549616](../images/image-20200731094549616.png)

这个delta term和我们之前学的a_j^{(l)}是对应的，那个a代表了某一个layer的某一个神经节点，对应的，这个符号就代表了某一个神经节点激活值的误差（激活函数是某一层到某一层的映射，所以说激活值应是某一个点经过激活函数所得的值）





![image-20200731103245042](../images/image-20200731103245042.png)



反向传播是从最后一层开始的，在计算的时候，我们从最后一层即layer4开始

![image-20200731103446053](../images/image-20200731103446053.png)

* delta 4这个式子可以用向量化的方式计算

* delta 3和delta 2中对g的求导可得![image-20200731103533277](../images/typora-user-images\image-20200731103533277.png)

* 没有对于delta 1的求解，因为delta 1是我们观察到的，没有误差的

通过delta我们可以很快的得出偏导值（暂时忽略了正则项）

![image-20200731103812061](../images/image-20200731103812061.png)

**进一步解释**

首先再详细解释前向传播

![image-20200731105212034](../images/image-20200731105212034.png)

![CF539F128FC5E0A94B29E2580DD378C5](../images/CF539F128FC5E0A94B29E2580DD378C5.png)



（啊，不过这个权重矩阵几维我还没整理清楚）



![image-20200731110446969](../images/image-20200731110446969.png)

训练样本(x(i),y(i))

为了更好的理解反向传播，这里以二元分类为例，此时代价函数里对K的求和就可以省略（K=1），同时y={0,1}，故这里的向量y实际上是实数，再进一步忽略正则化，置lambda=0，此时代价函数简化

那么第i个样本的代价为（相当于预测样本值和真值的相近程度）：

![image-20200731110946096](../images/image-20200731110946096.png)



而反向传播中的delta实际上是代价函数关于z的偏导数![image-20200731111227956](../images/image-20200731111227956.png)





![F5BF54CB480DBF2A08D843DF1CD67081](../images/F5BF54CB480DBF2A08D843DF1CD67081.png)

可见反向传播中并不需要考虑偏置项



### 梯度检测 

用以保证前向传播和反向传播的正确性

![image-20200801205402086](../images/image-20200801205402086.png)

![image-20200801205437034](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200801205437034.png)

做了一个数值上的逼近，也就是用两个红点连成直线的斜率近似在theta这一点的实际斜率

（双侧差分）

将特殊情况拓展到一般情况，此时theta为向量：

![image-20200801205701000](../images/image-20200801205701000.png)



一些总结：

* 用此方法得到的gradApprox和反向传播得到的导数DVec相比较
* 进行训练之前需要关掉梯度检测，该方法仅仅是为了确保反向传播算法实现正确



### 随机初始化

梯度下降算法中为theta设置初始值



对称权重问题：

在训练网络时若将theta初始化为0

![image-20200801211017179](../images/image-20200801211017179.png)

theta=0，故每一条边的权重都是相等的，即得到隐藏层的每一个元素的值都是相等的

同样偏导数也是相等的，最终这些隐藏单元都会得到相同的结果，最终只会得到一个特征，高度冗余

解决方案：

![image-20200802152100380](../images/image-20200802152100380.png)

将该矩阵每一个元素的值赋为这个区间的某一个数





### 总结



* 选择神经网络结构
  * 隐藏层：一般一直有单层，多层时常常有相同数目的单元项
* 训练神经网络
  * 初始化权重
  * 前项传播计算hx
  * 计算代价函数
  * 反向传播算法计算代价函数的偏导数
  * 梯度检测做比较
  * 最优化算法最小化代价函数（这里是非凸函数）



## 机器学习细节

如何改进算法？如何评估机器学习算法？



### 评估假设

判断过拟合：对于特征比较少的，可以绘制代价函数的曲线，但对于特征很多的，画图是不太实际的

方法：将数据分为训练集和测试集7:3，使用训练集计算出theta，使用测试集评估误差



### 模型选择和训练·····

题目有点长，不打了

如何更好的评估泛化能力，即在每一个数据集中得到更好的性能？

我们利用测试集得到代价函数，并选择代价最小的模型，但是这仅仅能够说明该模型在测试集上的表现比较好。

方法：将数据分为训练集、交叉验证集（cross validation）、测试集 6:2:2



![image-20200803153346009](../images/image-20200803153346009.png)

![image-20200803153542473](../images/image-20200803153542473.png)

利用交叉测试集去选择代价最小的

总的来说：

* 训练集用来训练模型，在梯度下降过程中确定模型的权重等参数

* 验证集用于模型的选择，不参与梯度下降
* 测试集只使用一次，在训练完成后用于评价最终的模型



### 诊断偏差和方差

![image-20200803154253465](../images/image-20200803154253465.png)

Jcv是交叉验证误差，Jtrain是训练集得到的误差

![image-20200803155018023](../images/image-20200803155018023.png)

高偏差high bias:欠拟合，此时训练误差和交叉验证误差都很大

高方差high variance：过拟合，此时训练误差小而交叉验证误差大



### 正则化和偏差、方差

正则化可以防止过拟合，那么它和偏差方差的关系如何呢

![image-20200803155748869](../images/image-20200803155748869.png)

![image-20200803155756489](C:\Users\THINKPAD\AppData\Roaming\Typora\typora-user-images\image-20200803155756489.png)

从中我们可以看到当lambda很大的时候，对各个参数的惩罚很重，因此导致各个theta趋近为0，造成欠拟合。同理lambda很小的时候会造成过拟合。

如何选择适合的lambda？

![image-20200803162112969](../images/image-20200803162112969.png)



这里的式子中并没有用正则项，其意义可能是正则项的引入本身是为了防止过拟合，选择更好的模型，当仅仅考虑lambda的时候其实是与选择模型这个工作无关的



### 学习曲线

![image-20200803164514317](../images/image-20200803164514317.png)

关于学习曲线：

* 当数据量比较小的时候，泛化能力比较差，因此交叉验证误差会随着数据量增加逐步减小
* 当数据量越来越大的时候，对于一个选定的模型，它拟合数据的程度会逐渐下降（因为很难满足所有数据的情况），因此训练误差会随着数据量增加逐步增大





高偏差的情况：项数少，故泛化能力比较强，但是训练误差会非常大，且两条线趋向于平坦，在后期即使增加数据量也不会有所作用

![image-20200803164314484](../images/image-20200803164314484.png)

![](../images/image-20200803163938372.png)

高方差的情况：泛化能力差，故交叉误差比较大，而随着数据量增大，训练误差也会稍稍增大



![image-20200803164201241](../images/image-20200803164201241.png)

此时增加样本也是有效的

### 判断总结

这一小节老师讲了讲什么情况下什么处理方法是正确的

fixes high variance

* get more training examples
* fixes high variance

fixes high bias:  model too simple

* try getting additional features
* try adding polynomial features
* try decreasing lambda



## 机器学习系统设计

### 确定执行的优先级

以区分垃圾邮件分类器为例

特征变量：一些特殊的单词

巴拉巴拉吧老师告诉你你可以自己提出多种方案，但是选择哪一个呢



### 误差分析

观察被错误分类的邮件，或许能总结出新特征

或者使用一些数值评估的方式进行分析，用一个实数去评估算法的性能（比如说错误类）



### 不对称分类的误差评估

偏斜类：某一个类的样本数比另一个类多得多

比如说在肿瘤分类的例子中，可能只有百分之0.5的病人是恶性的，此时如果仅仅用错误率来评估算法性能就不科学了



![image-20200803210409432](../images/image-20200803210409432.png)

因此引入其他的评估指标：

* 查准率/召回率

  * 查准率(Precison)：对于我们预测y=1即患有癌症的病人，有多大比例是真正患癌的

    True positives/predicted postives

  * 召回率(Recall)：对于所有得了癌症的人，我们有多大概率预测正确

    True positives/actual positives

注意：

* 好的分类模型：搞查准率和召回率
* 稀少情况设置为y=1，如得癌症



### 精准率和召回率的权衡

如果我们想要提高准确率，更改阈值，使得hx>=0.7的时候才预测y=1（原来hx>=0.5），此时我们会有更高的查准率和更低的召回率（因为此时预测为y=1的人变少了）

反过来，如果我们更改阈值为，使得hx>=0.3的时候就预测y=1，此时就会有比较低的查准率以及更高的召回率了

那么，如何比较不同算法的精准率和召回率呢？

![image-20200803211912995](../images/image-20200803211912995.png)

只考虑平均值是不可行的，比如算法3中平均值最大，但是却有非常小的查准率

使用F_1值：2*PR/(P+R)

如果查准率P或者召回率R中其中一个比较小，F值就会比较小



### 机器学习数据

大量的数据样本是非常有用的，可以提升算法的性能，其前提如下：

特征x已经有足够的信息预测y



## 支持向量机













## 存疑

* 极大似然估计部分后期可以补一下

* 凸优化

* ~~！！！注意这里的theta更新过程是可以用向量化而非loop直接实现的，但是昨天那个为啥子算不对，有点坑爹~~

* 实验里面为啥要转矩阵形式，或者说为啥动不动就变成了向量(这个应该和梯度计算方法有关)

* 神经网络里那个权重矩阵怎么算的

* 反向传播偏导数

* 雅克布矩阵https://www.cnblogs.com/tongtong123/p/10634716.html

* minimize的具体方式

  
  
  



参考

> ​	https://blog.csdn.net/qq_29317617/article/details/86312154







## 实验





### ex3

注意：

* theta0不应该做正则化处理
* 在多分类器时，传入参数y应该根据当前label情况映射为一个1或0的数组
* “invalid gradient vector from minimized function”  运行oneVsAll函数的时候，传入参数theta0应该是一个一维函数











